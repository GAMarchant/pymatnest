################################################################################
Running:

   ns_run < inputs

Example input files for clusters and periodic, MC and MD, are in the tests/ directory
See also
    ns_run --help

If you get weird errors about modules and/or .so files not found, do (in sh syntax)
   export PYTHONPATH=ns_run_dir:$PYTHONPATH
where "ns_run_dir" is the directory where ns_run is.
This appears to be necessary on some HPC machines where mpirun copies the executable,
    because ns_run by default looks for modules in the same directory as the top
    level python script itself

################################################################################
Analysis
   ns_analyze -M 0.01 -D 0.01 -n 100 file.energies > analysis

see also
   ns_analyze --help

################################################################################
Compilation of the streamlined FORTRAN models and MC/MD walkers

1. edit Makefile.arch (by default Linux appropriate variables are set)
2. make (or if you don't have pdflatex, make libs)

################################################################################
Using QUIP

Make sure PYTHONPATH is set correctly to find the quippy module

Examples are in tests_quip/ directory

################################################################################
Using LAMMPS

Apply the MPI patch to the LAMMPS source by doing
    cd lammps_top_dir/src
    patch < ns_run_dir/lammps.patch
cd lammps_top_dir/src
Create a Makefile for _parallel_ lammps in MAKE/MINE, but define "-DLIBRARY_MPI_COMM_WORLD=MPI_COMM_SELF" 
    in the LMP_INC makefile variable.
make machine mode=shlib
copy the resulting liblammps_machine.so to someplace in your LD_LIBRARY_PATH.  Copying it to ns_run_dir 
    may also work, especially if that's also where you put lammps.py
copy lammps_dir/python/lammps.py into somplace in your PYTHONPATH.  Copying it to ns_run_dir should also work.

Examples are in the tests_lammps/directory.  LAMMPS_name is what you set for "machine" when compiling LAMMPS.

Note that "serial" LAMMPS still links to fake MPI routines, which then conflict in unpredictable ways with 
    the true mpi routines that mpi4py includes.

I expect that OpenMP parallelized LAMMPS will work OK, but this is completely untested.

LAMMPS ASE interface is a heavily modified version of
   https://svn.fysik.dtu.dk/projects/ase-extra/trunk/ase/calculators/lammpslib.py

