# To run with N MPI tasks, each OpenMP parallelized over M threads (bash and openmpi syntax)
# request N*M slots, make sure number of cores on a node is divisible by M
#    export OMP_NUM_THREADS=M
#    mpirun -x OMP_NUM_THREADS -np N --map-by slot:pe=$OMP_NUM_THREADS  ns_run < inputs

max_volume_per_atom=1000000.0

start_species=1 64 1.0
start_energy_ceiling=100.0

n_walkers=256
n_cull=1
n_iter_per_walker=5000
out_file_prefix=test.periodic.MD.lammps_omp

################################################################################

atom_algorithm=MD
n_steps_expected=64
atom_n_substeps=8
cell_n_substeps=2

MD_atom_timestep=0.01
MD_atom_timestep_max=0.025

################################################################################
MC_cell_P=2.37e-4

MC_cell_volume_per_atom_step_size=100.0
MC_cell_volume_per_atom_step_size_max=5000.0

MC_cell_shear_step_size=0.01
MC_cell_shear_step_size_max=0.1

MC_cell_stretch_step_size=0.01
MC_cell_stretch_step_size_max=0.1

MC_cell_min_aspect_ratio=0.9
################################################################################

LAMMPS_header_extra=package omp 0
LAMMPS_init_cmds=pair_style lj/cut/omp 9.00; pair_coeff * * 0.1 3.0; pair_modify shift yes
LAMMPS_name=tin_comm_self

energy_calculator=lammps

rng=numpy

debug=0
